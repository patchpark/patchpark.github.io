---
title: kNN
date: 2024-03-23 23:00:00 +09:00
categories: [cs, 기계 학습]
---

# kNN
매우 기본적인 Classification 알고리즘이다. k-Nearest Neighbor 알고리즘은, 새로 입력된 데이터에 대해, 데이터와 가장 가까운 k개의 데이터를 선택한 뒤 k개의 데이터의 종류를 분석해 가장 많은 종류로 분류하는 방식으로 작동한다. 동률을 피하기 위해 보통 k는 홀수로 지정해준다. 데이터의 종류가 여러개인 경우, 중립 구역이 발생할 수도 있다.

![kNN_01](https://github.com/patchpark/patchpark.github.io/assets/116805893/2130ceb1-eccc-4f38-aa4c-e8c04533c64b)


# 거리 측정법
입력된 데이터와 가까운 데이터들을 찾기 위해서는 두 점 사이의 거리를 알아야한다. hamming distance, cosine similarity 등등 다양한 거리 측정법이 있지만, 대표적인 거리 측정법으로 Lp 거리가 존재한다. Lp 거리란 두 점의 각 좌표의 차의 p제곱 후 모두 더한 뒤 p루트를 해주는 것이다. 예를 들어 (3,4)와 (5,2)의 L1 거리는 |3-5| + |4-2| = 4 이고, L2거리는 √((3 - 5)^2 + (4 - 2)^2) = 2 √2이다.

# Hyper parameter
(trainable parameter)와 대비되는 개념이다. kNN 알고리즘에서의 k와 같이 학습을 통해 얻어지는 것이 아닌, 우리가 설정해 줘야 하는 값이다. 하이퍼 파라미터를 설정하는데에는 몇가지 방식이 있다.

1. 가지고 있는 데이터에 가장 잘 맞는 파라미터 설정(k = 1)
2. 데이터를 학습과 테스트로 나눠, 테스트 데이터에 대해 가장 잘 동작하는 파라미터 설정
3. 데이터를 학습, 검증, 테스트로 나눠 검증에서 오류가 최소가 되는 파라미터 설정 후 테스트
4. 3과 동일하지만 학습과 검증을 여러가지로 돌려가며 테스트


![kNN_02](https://github.com/patchpark/patchpark.github.io/assets/116805893/eb2a1f9b-13bd-445c-b00f-5efdf3ebed0a)


1,2는 새로운 데이터에 대해서 어떻게 작동할지 모르기 때문에 잘 쓰이지 않고, 제일 정확한 것은 4이지만, 딥러닝 등의 경우 데이터가 너무 많기에 데이터 쏠림으로 인한 편중이 잘 발생하지 않아 3을 많이 이용한다.
